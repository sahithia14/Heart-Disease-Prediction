{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data analysis\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# for data visuals\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "# for machine learning \n",
    "from sklearn.ensemble import RandomForestClassifier as RFclassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier as ADclassifier\n",
    "from sklearn.ensemble import BaggingClassifier as BGclassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ETclassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBclassifier\n",
    "#from sklearn.ensemble import IsolationForest as IFclassifier\n",
    "#from sklearn.ensemble import RandomTreesEmbedding as RTclassifier\n",
    "#from sklearn.ensemble import VotingClassifier as VOclassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('processed_cleveland_data_train.csv')\n",
    "testData = pd.read_csv('processed_cleveland_data_test.csv')\n",
    "\n",
    "\n",
    "def replace_predict(df):\n",
    "    df['num'] = df['num'].replace([1, 2, 3, 4, 5, 6], 1)\n",
    "# replacing anything greater than 1 with 1 because it is a binary classification problem\n",
    "\n",
    "       \n",
    "replace_predict(trainData)\n",
    "replace_predict(testData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.barplot(x='chol', y='num', palette=\"rocket\", data=trainData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the train and test data sets are further seprated by thier features and the predicted diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtrain = trainData.drop(['num'], axis=1)\n",
    "Ytrain = trainData['num']\n",
    "\n",
    "Xtest = testData.drop(['num'], axis=1)\n",
    "Ytest = testData['num']\n",
    "\n",
    "Ytrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Create all the model\n",
    "\n",
    "rfModel=RFclassifier()\n",
    "adModel= ADclassifier()\n",
    "bgModel=BGclassifier()\n",
    "etModel=ETclassifier()\n",
    "gbModel=GBclassifier()\n",
    "#ifModel=IFclassifier()\n",
    "# rtModel=RTclassifier()\n",
    "#voModel= VOclassifier(50)\n",
    "\n",
    "\n",
    "# Choose parameters for all model\n",
    "# rfparameters = {'n_estimators': [4,6,9,13,18],\n",
    "#              'max_features': ['log2', 'sqrt', 'auto'],\n",
    "#              'criterion': ['entropy', 'gini'],\n",
    "#              'max_depth': [1,16,32,32,26],\n",
    "#              'min_samples_split': [2, 3, 5, 8,12],\n",
    "#              'min_samples_leaf': [1, 2, 8, 10, 15]}\n",
    "# # adparameters = {'n_estimators': [10,20,50,60,70],'base_estimator':None, 'learning_rate':[.7,1.,1.5],\n",
    "#                'algorithm'=’SAMME.R’, 'random_state'=None}\n",
    "# bgparameters = {'n_estimators': [10,20,50,60,70,4,6,9,13,18], \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Type of scoring to compare parameter combos \n",
    "# acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# # Run grid search \n",
    "# # grid search is an algorthim which automatically finds the best paramters for a particular model\n",
    "# grid_obj = GridSearchCV(rfModel, rfparameters, scoring=acc_scorer)\n",
    "# grid_obj = grid_obj.fit(Xtrain, Ytrain)\n",
    "\n",
    "# # Pick the best combination of parameters\n",
    "# rfModel = grid_obj.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best algorithm to the data, and decides choosen parameters \n",
    "rfModel.fit(Xtrain, Ytrain)\n",
    "adModel.fit(Xtrain, Ytrain)\n",
    "bgModel.fit(Xtrain, Ytrain)\n",
    "etModel.fit(Xtrain, Ytrain)\n",
    "gbModel.fit(Xtrain, Ytrain)\n",
    "#ifModel.fit(Xtrain, Ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Model using accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalaccu=[]\n",
    "models=[rfModel,adModel,bgModel,etModel,gbModel]\n",
    "count=0\n",
    "while count<5:\n",
    "    predictions = models[count].predict(Xtest)\n",
    "    print(models[count])\n",
    "    print(predictions)\n",
    "    accuracy= accuracy_score(Ytest, predictions)\n",
    "    count=count+1\n",
    "    finalaccu.append(accuracy)\n",
    "print(finalaccu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(true_label, predicted_prob):\n",
    "   if true_label == 1:\n",
    "     return -log(predicted_prob)\n",
    "   else:\n",
    "    return -log(1 - predicted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Model using confidence and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "finalconf=[]\n",
    "models=[rfModel,adModel,bgModel,etModel,gbModel]\n",
    "count=0\n",
    "while count<5:\n",
    "    confidence = models[count].predict_proba(Xtest)\n",
    "    probs = confidence[:, 1]\n",
    "    #print(confidence)\n",
    "    #print(probs)\n",
    "    #print(Ytest)\n",
    "    #print(Ytest.shape)\n",
    "    #print(Ytest.size)\n",
    "    # calculate log loss\n",
    "    loss = log_loss(Ytest.values, probs)\n",
    "    print(loss)\n",
    "    #print(models[count])\n",
    "    #print(confidence) \n",
    "    #print(Ytest.values)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross Validation with KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "alldata = pd.read_csv('processed_cleveland_data.csv')\n",
    "replace_predict(alldata)\n",
    "\n",
    "Xall = alldata.drop(['num'], axis=1)\n",
    "Yall = alldata['num']\n",
    "\n",
    "def run_kfold(model):\n",
    "    kf = KFold(n_splits=5)\n",
    "    outcomes = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(Xall):\n",
    "        fold += 1\n",
    "        Xtrain, Xtest = Xall.values[train_index], Xall.values[test_index]\n",
    "        Ytrain, Ytest = Yall.values[train_index], Yall.values[test_index]\n",
    "        model.fit(Xtrain, Ytrain)\n",
    "        #print (Xtrain.shape)\n",
    "        #print (Ytrain.shape)\n",
    "        #print(Xtrain[0])\n",
    "        print(Ytrain[0])\n",
    "        predictions = model.predict(Xtest)\n",
    "        print(predictions)\n",
    "        accuracy = accuracy_score(Ytest, predictions)\n",
    "        print(accuracy)\n",
    "        outcomes.append(accuracy)\n",
    "        print(outcomes)\n",
    "        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy)) \n",
    "        mean_outcome = np.mean(outcomes)\n",
    "        print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n",
    "        \n",
    "run_kfold(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
